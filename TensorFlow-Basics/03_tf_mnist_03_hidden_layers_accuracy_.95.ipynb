{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of hand-written dataset MNIST using 2 hidden layer network and relu as the activation function\n",
    "### Accuracy ~ 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST\\train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# import \n",
    "import tensorflow as tf\n",
    "\n",
    "# import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/MNIST\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy  0.9519\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "num_classes = 10\n",
    "img_size = 28 \n",
    "img_size_flat = img_size * img_size # 784\n",
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "\n",
    "# hidden layers parameters\n",
    "hidden_1 = 2048 # number of features in hidden layer 1\n",
    "hidden_2 = 2048 # number of features in hidden layer 2\n",
    "\n",
    "''' \"matrix_multiplication\" layer by layer explained-->\n",
    "\n",
    "# input_layer  ==>             \n",
    "x_data [batch_size,784]\n",
    "\n",
    "# hidden_layer_1  ==>  \n",
    "x_data[batch_size,784]  'mat_mul'  w1[784,2048]  +  b1[2048]  ==> output_of_hidden_layer_1[batch_size, 2048]\n",
    "\n",
    "# hidden_layer_2  ==>  \n",
    "output_of_hidden_layer_1[batch_size,2048]  'mat_mul'  w2[2048,2048]  +  b2[2048] ==> output_of_hidden_layer_1[batch_size,2048]\n",
    "\n",
    "#output_layer(final) ==>\n",
    "output_of_hidden_layer_1[batch_size,2048] 'mat_mul'  out_w[2048,10]  +  out_b[10]  ==>  final prediction[batch_size, 10]\n",
    "'''\n",
    "\n",
    "# input data x\n",
    "x_data = tf.placeholder(tf.float32, [None, img_size_flat], name=\"x_data\") # shape = (batch_size, 784)\n",
    "\n",
    "# input data labels \n",
    "x_labels = tf.placeholder(tf.float32, [None, num_classes], name=\"x_labels\") # shape = (batch_size, 10)\n",
    "\n",
    "# random weights and biases \n",
    "# we will make them Variable because we have to initialize with random values and then later train them\n",
    "weights = {'w1': tf.Variable(tf.random_normal([img_size_flat,hidden_1]), name=\"weights1\"),\n",
    "          'w2': tf.Variable(tf.random_normal([hidden_1, hidden_2]), name=\"weights2\"),\n",
    "          'out': tf.Variable(tf.random_normal([hidden_2, num_classes]), name=\"weights_out\")}\n",
    "\n",
    "biases = {'b1': tf.Variable(tf.random_normal([hidden_1]),name=\"biases1\"),\n",
    "         'b2': tf.Variable(tf.random_normal([hidden_2]),name=\"biases2\"),\n",
    "         'out': tf.Variable(tf.random_normal([num_classes]),name=\"biases_out\")}\n",
    "\n",
    "\n",
    "# creating the network with 2 hidden layers and activation function as 'Relu'\n",
    "\n",
    "# input layer:\n",
    "# data will be feed in batches\n",
    "\n",
    "# layer_1:\n",
    "layer_1 = tf.add(tf.matmul(x_data, weights['w1']), biases['b1'])\n",
    "layer_1 = tf.nn.relu(layer_1, name=\"layer_1\")\n",
    "\n",
    "# layer_2:\n",
    "layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "layer_2 = tf.nn.relu(layer_2, name=\"layer_2\")\n",
    "\n",
    "# output layer:\n",
    "# we don't apply any activation here\n",
    "logits = tf.add(tf.matmul(layer_2, weights['out']) , biases['out'], name=\"logits\")\n",
    "\n",
    "\n",
    "# from here it's gonna be same as single layer network\n",
    "\n",
    "# introduce softmax non-linearity\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=x_labels, logits=logits, name=\"cross_entropy\")\n",
    "\n",
    "# sum of all the cross_entropy is the cost of our model\n",
    "cost = tf.reduce_mean(cross_entropy, name=\"cost\")\n",
    "\n",
    "#optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "### Now that the computation graph is ready, we need to run it using tf.Session()\n",
    "\n",
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# define a session with default graph\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for _ in range(training_epochs):\n",
    "    batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "    sess.run(fetches=optimizer, feed_dict={x_data:batch_x, x_labels:batch_y})\n",
    "    \n",
    "how_many_correct = tf.equal(tf.argmax(x_labels, axis=1), tf.argmax(logits, axis=1), name=\"howmanycorrect\")\n",
    "\n",
    "# accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(how_many_correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "# print final accuracy on test data\n",
    "print(\"accuracy \", sess.run(fetches=accuracy, feed_dict={x_data:mnist.test.images, x_labels:mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
